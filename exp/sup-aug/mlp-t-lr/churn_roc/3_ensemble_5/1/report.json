{
    "program": "bin/ensemble.py",
    "single_program": "/slot/sandbox/d/in/script/0_script_unpacked/pretrains/bin/pretrain_mask_supervised___67e33d82bab74a0c9b33cc2251e2e617.py",
    "config": {
        "seeds": [
            "5",
            "6",
            "7",
            "8",
            "9"
        ]
    },
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.8492116739349211,
                "recall": 0.9935243328100472,
                "f1-score": 0.9157171278712244,
                "support": 5096
            },
            "1": {
                "precision": 0.9246575342465754,
                "recall": 0.31058282208588955,
                "f1-score": 0.4649827784156142,
                "support": 1304
            },
            "accuracy": 0.854375,
            "macro avg": {
                "precision": 0.8869346040907482,
                "recall": 0.6520535774479683,
                "f1-score": 0.6903499531434193,
                "support": 6400
            },
            "weighted avg": {
                "precision": 0.8645837679734207,
                "recall": 0.854375,
                "f1-score": 0.8238800041696437,
                "support": 6400
            },
            "roc_auc": 0.8919710876327879,
            "score": 0.8919710876327879
        },
        "val": {
            "0": {
                "precision": 0.8453333333333334,
                "recall": 0.9952904238618524,
                "f1-score": 0.9142033165104542,
                "support": 1274
            },
            "1": {
                "precision": 0.94,
                "recall": 0.2883435582822086,
                "f1-score": 0.4413145539906103,
                "support": 326
            },
            "accuracy": 0.85125,
            "macro avg": {
                "precision": 0.8926666666666667,
                "recall": 0.6418169910720305,
                "f1-score": 0.6777589352505322,
                "support": 1600
            },
            "weighted avg": {
                "precision": 0.8646216666666667,
                "recall": 0.85125,
                "f1-score": 0.817852231147036,
                "support": 1600
            },
            "roc_auc": 0.8734385684429506,
            "score": 0.8734385684429506
        },
        "test": {
            "0": {
                "precision": 0.8421891604675876,
                "recall": 0.9949780288763339,
                "f1-score": 0.9122302158273381,
                "support": 1593
            },
            "1": {
                "precision": 0.9322033898305084,
                "recall": 0.2702702702702703,
                "f1-score": 0.419047619047619,
                "support": 407
            },
            "accuracy": 0.8475,
            "macro avg": {
                "precision": 0.8871962751490481,
                "recall": 0.632624149573302,
                "f1-score": 0.6656389174374786,
                "support": 2000
            },
            "weighted avg": {
                "precision": 0.860507056142942,
                "recall": 0.8475,
                "f1-score": 0.8118675573826653,
                "support": 2000
            },
            "roc_auc": 0.8647245087923053,
            "score": 0.8647245087923053
        }
    }
}
