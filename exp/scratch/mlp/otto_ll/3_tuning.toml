seed = 0
program = "bin/finetune.py"
n_trials = 100

[base_config]
seed = 0

[base_config.data]
T_cache = true
path = "data/otto_ll"

[base_config.data.T]
normalization = "__none__"

[base_config.training]
batch_size = 256

[base_config.model]
kind = "mlp"

[space.model.config]
d_layers = [
    "$fixed_mlp_d_layers",
    1,
    8,
    512,
]
dropout = [
    "?uniform",
    0.0,
    0.0,
    0.5,
]

[space.training]
lr = [
    "loguniform",
    5e-05,
    0.005,
]
weight_decay = [
    "?loguniform",
    0.0,
    1e-06,
    0.001,
]
