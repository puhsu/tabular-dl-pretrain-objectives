{
    "program": "bin/ensemble.py",
    "single_program": "/slot/sandbox/d/in/script/0_script_unpacked/pretrains/bin/pretrain_ae___dbf3b5cba2e54f98879532c99ffbffda.py",
    "config": {
        "seeds": [
            "5",
            "6",
            "7",
            "8",
            "9"
        ]
    },
    "prediction_type": "probs",
    "metrics": {
        "train": {
            "0": {
                "precision": 0.8842275837234426,
                "recall": 0.9636970172684458,
                "f1-score": 0.9222535211267605,
                "support": 5096
            },
            "1": {
                "precision": 0.7813238770685579,
                "recall": 0.5069018404907976,
                "f1-score": 0.6148837209302326,
                "support": 1304
            },
            "accuracy": 0.870625,
            "macro avg": {
                "precision": 0.8327757303960002,
                "recall": 0.7352994288796217,
                "f1-score": 0.7685686210284965,
                "support": 6400
            },
            "weighted avg": {
                "precision": 0.8632609534925099,
                "recall": 0.870625,
                "f1-score": 0.8596269243367181,
                "support": 6400
            },
            "roc_auc": 0.8870374394448671,
            "score": 0.8870374394448671
        },
        "val": {
            "0": {
                "precision": 0.8841726618705036,
                "recall": 0.9646781789638933,
                "f1-score": 0.9226726726726727,
                "support": 1274
            },
            "1": {
                "precision": 0.7857142857142857,
                "recall": 0.5061349693251533,
                "f1-score": 0.6156716417910447,
                "support": 326
            },
            "accuracy": 0.87125,
            "macro avg": {
                "precision": 0.8349434737923946,
                "recall": 0.7354065741445233,
                "f1-score": 0.7691721572318587,
                "support": 1600
            },
            "weighted avg": {
                "precision": 0.8641117677286743,
                "recall": 0.87125,
                "f1-score": 0.860121212630541,
                "support": 1600
            },
            "roc_auc": 0.8784202213211855,
            "score": 0.8784202213211855
        },
        "test": {
            "0": {
                "precision": 0.8740022805017104,
                "recall": 0.9623352165725048,
                "f1-score": 0.9160442187033164,
                "support": 1593
            },
            "1": {
                "precision": 0.7560975609756098,
                "recall": 0.457002457002457,
                "f1-score": 0.5696784073506891,
                "support": 407
            },
            "accuracy": 0.8595,
            "macro avg": {
                "precision": 0.8150499207386601,
                "recall": 0.7096688367874808,
                "f1-score": 0.7428613130270028,
                "support": 2000
            },
            "weighted avg": {
                "precision": 0.8500086700781488,
                "recall": 0.8595,
                "f1-score": 0.8455587760930569,
                "support": 2000
            },
            "roc_auc": 0.8651579160053736,
            "score": 0.8651579160053736
        }
    }
}
