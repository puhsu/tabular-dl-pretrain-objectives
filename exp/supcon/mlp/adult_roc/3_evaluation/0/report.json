{
    "program": "/slot/sandbox/d/in/script/0_script_unpacked/pretrains/bin/pretrain_supervised_contrastive___63bde84a6a9c47e79303f92633185683.py",
    "environment": {
        "CUDA_VISIBLE_DEVICES": "0",
        "gpus": {
            "driver": "450.119.04",
            "devices": [
                {
                    "name": "A100-SXM-80GB",
                    "memory_total": 85199093760,
                    "memory_free": 85195948032,
                    "memory_used": 3145728,
                    "utilization": 0
                }
            ]
        },
        "torch.version.cuda": "11.1",
        "torch.backends.cudnn.version()": 8005,
        "torch.cuda.nccl.version()": [
            2,
            10,
            3
        ]
    },
    "config": {
        "seed": 0,
        "data": {
            "path": "data/adult_roc",
            "T": {
                "seed": 0,
                "normalization": "quantile",
                "num_nan_policy": null,
                "cat_nan_policy": null,
                "cat_min_frequency": null,
                "cat_encoding": null,
                "y_policy": "default"
            },
            "T_cache": true
        },
        "model": {
            "kind": "mlp",
            "config": {
                "d_layers": [
                    512,
                    512,
                    512,
                    512
                ],
                "dropout": 0.2376316575734998
            },
            "default": true,
            "checkpoint": null,
            "num_embedding_arch": [],
            "d_num_embedding": null,
            "positional_encoding": null,
            "d_cat_embedding": null
        },
        "training": {
            "batch_size": 256,
            "lr": 7.500240939498116e-05,
            "weight_decay": 0.0,
            "optimizer": "AdamW",
            "patience": 16,
            "n_epochs": Infinity,
            "eval_batch_size": 8192
        },
        "pretrain": {
            "corrupt_probability": 0.780250346955173,
            "corrupt_strategy": "resample",
            "d_hidden_head": 512,
            "lr": 7.500240939498116e-05,
            "weight_decay": 0.0,
            "patience": 2,
            "n_iterations": 100000,
            "finetune_every": 10000,
            "replace_strategy": "shuffle"
        },
        "bins": null
    },
    "epoch_size": 102,
    "n_parameters": 1369088,
    "metrics": {
        "iteration_scores": {
            "10000": {
                "train": {
                    "score": 0.9317347700285956,
                    "pretrain_loss": 10.874069213867188
                },
                "val": {
                    "score": 0.9150449588328757,
                    "pretrain_loss": 11.135788917541504
                },
                "test": {
                    "score": 0.9103760877415394,
                    "pretrain_loss": 11.503812789916992
                }
            },
            "20000": {
                "train": {
                    "score": 0.9290992903384822,
                    "pretrain_loss": 10.874124526977539
                },
                "val": {
                    "score": 0.9154964817069395,
                    "pretrain_loss": 11.13697624206543
                },
                "test": {
                    "score": 0.9115321146822553,
                    "pretrain_loss": 11.50450325012207
                }
            },
            "30000": {
                "train": {
                    "score": 0.9320822427827165,
                    "pretrain_loss": 10.872722625732422
                },
                "val": {
                    "score": 0.9158227750149605,
                    "pretrain_loss": 11.135061264038086
                },
                "test": {
                    "score": 0.9112544252473758,
                    "pretrain_loss": 11.50335693359375
                }
            },
            "40000": {
                "train": {
                    "score": 0.9352703043948712,
                    "pretrain_loss": 10.871252059936523
                },
                "val": {
                    "score": 0.9159903066383277,
                    "pretrain_loss": 11.13425064086914
                },
                "test": {
                    "score": 0.9107537980650711,
                    "pretrain_loss": 11.502731323242188
                }
            },
            "50000": {
                "train": {
                    "score": 0.9397039345272609,
                    "pretrain_loss": 10.869719505310059
                },
                "val": {
                    "score": 0.9155298203710199,
                    "pretrain_loss": 11.133953094482422
                },
                "test": {
                    "score": 0.9096407925476652,
                    "pretrain_loss": 11.502554893493652
                }
            },
            "60000": {
                "train": {
                    "score": 0.9411669259401005,
                    "pretrain_loss": 10.868934631347656
                },
                "val": {
                    "score": 0.9145201811766163,
                    "pretrain_loss": 11.134232521057129
                },
                "test": {
                    "score": 0.9087693551972075,
                    "pretrain_loss": 11.50234603881836
                }
            },
            "70000": {
                "train": {
                    "score": 0.9438212933925281,
                    "pretrain_loss": 10.86822509765625
                },
                "val": {
                    "score": 0.9139462660696229,
                    "pretrain_loss": 11.13357925415039
                },
                "test": {
                    "score": 0.9078450689294157,
                    "pretrain_loss": 11.502449989318848
                }
            }
        },
        "train": {
            "score": 0.9352703043948712
        },
        "val": {
            "score": 0.9159903066383277
        },
        "test": {
            "score": 0.9107537980650711
        }
    },
    "best_iteration": 40000,
    "time": "0:09:35"
}
